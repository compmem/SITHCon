{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:36:37.390727Z",
     "start_time": "2021-05-03T01:36:37.208757Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:36:37.943324Z",
     "start_time": "2021-05-03T01:36:37.391688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set_context(\"poster\")\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "ttype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "ctype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "print(ttype)\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import gridspec\n",
    "from sithcon import SITHCon_Layer, _SITHCon_Core, iSITH\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import itertools\n",
    "from csv import DictWriter\n",
    "import os \n",
    "from os.path import join\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from math import factorial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:08:04.831113Z",
     "start_time": "2021-05-03T01:08:04.827482Z"
    }
   },
   "outputs": [],
   "source": [
    "MORSE_CODE_DICT = { 'A':'.-', 'B':'-...', \n",
    "                    'C':'-.-.', 'D':'-..', 'E':'.', \n",
    "                    'F':'..-.', 'G':'--.', 'H':'....', \n",
    "                    'I':'..', 'J':'.---', 'K':'-.-', \n",
    "                    'L':'.-..', 'M':'--', 'N':'-.', \n",
    "                    'O':'---', 'P':'.--.', 'Q':'--.-', \n",
    "                    'R':'.-.', 'S':'...', 'T':'-', \n",
    "                    'U':'..-', 'V':'...-', 'W':'.--', \n",
    "                    'X':'-..-', 'Y':'-.--', 'Z':'--..', \n",
    "                    '1':'.----', '2':'..---', '3':'...--', \n",
    "                    '4':'....-', '5':'.....', '6':'-....', \n",
    "                    '7':'--...', '8':'---..', '9':'----.', \n",
    "                    '0':'-----', ', ':'--..--', '.':'.-.-.-', \n",
    "                    '?':'..--..', '/':'-..-.', '-':'-....-', \n",
    "                    '(':'-.--.', ')':'-.--.-'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:08:04.845819Z",
     "start_time": "2021-05-03T01:08:04.831952Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..--.. 1010111011101010\n",
      "\n",
      "[1 0 1 1 1 0 0 0] A\n",
      "[1 1 1 0 1 0 1 0 1 0 0 0] B\n",
      "[1 1 1 0 1 0 1 1 1 0 1 0 0 0] C\n",
      "[1 1 1 0 1 0 1 0 0 0] D\n",
      "[1 0 0 0] E\n",
      "[1 0 1 0 1 1 1 0 1 0 0 0] F\n",
      "[1 1 1 0 1 1 1 0 1 0 0 0] G\n",
      "[1 0 1 0 1 0 1 0 0 0] H\n",
      "[1 0 1 0 0 0] I\n",
      "[1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0] J\n",
      "[1 1 1 0 1 0 1 1 1 0 0 0] K\n",
      "[1 0 1 1 1 0 1 0 1 0 0 0] L\n",
      "[1 1 1 0 1 1 1 0 0 0] M\n",
      "[1 1 1 0 1 0 0 0] N\n",
      "[1 1 1 0 1 1 1 0 1 1 1 0 0 0] O\n",
      "[1 0 1 1 1 0 1 1 1 0 1 0 0 0] P\n",
      "[1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0] Q\n",
      "[1 0 1 1 1 0 1 0 0 0] R\n",
      "[1 0 1 0 1 0 0 0] S\n",
      "[1 1 1 0 0 0] T\n",
      "[1 0 1 0 1 1 1 0 0 0] U\n",
      "[1 0 1 0 1 0 1 1 1 0 0 0] V\n",
      "[1 0 1 1 1 0 1 1 1 0 0 0] W\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 0] X\n",
      "[1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0] Y\n",
      "[1 1 1 0 1 1 1 0 1 0 1 0 0 0] Z\n",
      "[1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0] 1\n",
      "[1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0] 2\n",
      "[1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0] 3\n",
      "[1 0 1 0 1 0 1 0 1 1 1 0 0 0] 4\n",
      "[1 0 1 0 1 0 1 0 1 0 0 0] 5\n",
      "[1 1 1 0 1 0 1 0 1 0 1 0 0 0] 6\n",
      "[1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0] 7\n",
      "[1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0] 8\n",
      "[1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0] 9\n",
      "[1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0] 0\n",
      "[1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0] , \n",
      "[1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0] .\n",
      "[1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0] ?\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0] /\n",
      "[1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0] -\n",
      "[1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0] (\n",
      "[1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0] )\n"
     ]
    }
   ],
   "source": [
    "morse_code_numpy = {key:np.array([int(x) for x in MORSE_CODE_DICT[key].replace('.', '10').replace('-', '1110')] + [0, 0])\n",
    "                    for key in MORSE_CODE_DICT.keys()}\n",
    "\n",
    "for k in morse_code_numpy.keys():\n",
    "    print(morse_code_numpy[k], k)\n",
    "subset = list(morse_code_numpy.keys())\n",
    "\n",
    "id2key = subset\n",
    "key2id = {}\n",
    "for idx, s in enumerate(subset):\n",
    "    key2id[s] = idx\n",
    "\n",
    "X = [ttype(morse_code_numpy[k])for k in subset]\n",
    "Y = torch.LongTensor(np.arange(0,len(X)))\n",
    "print(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:08:07.166363Z",
     "start_time": "2021-05-03T01:08:07.161989Z"
    }
   },
   "outputs": [],
   "source": [
    "class SITHCon_Classifier(nn.Module):\n",
    "    def __init__(self, out_classes, layer_params, \n",
    "                 act_func=nn.ReLU, batch_norm=False,\n",
    "                 dropout=.2):\n",
    "        super(SITHCon_Classifier, self).__init__()\n",
    "        last_channels = layer_params[-1]['channels']\n",
    "        self.transform_linears = nn.ModuleList([nn.Linear(l['channels'], l['channels'])\n",
    "                                                for l in layer_params])\n",
    "        self.sithcon_layers = nn.ModuleList([SITHCon_Layer(l, act_func) for l in layer_params])\n",
    "        self.to_out = nn.Linear(last_channels, out_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        x = inp\n",
    "        #out = []\n",
    "        for i in range(len(self.sithcon_layers)):\n",
    "            x = self.sithcon_layers[i](x)\n",
    "            \n",
    "            x = self.transform_linears[i](x[:,0,:,:].transpose(1,2))\n",
    "            x = x.unsqueeze(1).transpose(2,3)\n",
    "\n",
    "            #out.append(x.clone())\n",
    "        x = x.transpose(2,3)[:, 0, :, :]\n",
    "        #x = x.transpose(2,3)[:, 0, :, :]\n",
    "        x = self.to_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:18:07.788367Z",
     "start_time": "2021-05-03T01:18:07.691200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Weights: 33118\n",
      "SITHCon_Classifier(\n",
      "  (transform_linears): ModuleList(\n",
      "    (0): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  )\n",
      "  (sithcon_layers): ModuleList(\n",
      "    (0): SITHCon_Layer(\n",
      "      (sithcon): _SITHCon_Core(\n",
      "        (sith): iSITH(ntau=400, tau_min=0.1, tau_max=4000, buff_max=6500, dt=1, k=35, g=0.0)\n",
      "        (conv): Conv2d(1, 35, kernel_size=(1, 23), stride=(1, 1), dilation=(1, 2), bias=False)\n",
      "        (maxp): MaxPool1d(kernel_size=356, stride=356, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (act_func): ReLU()\n",
      "    )\n",
      "    (1): SITHCon_Layer(\n",
      "      (sithcon): _SITHCon_Core(\n",
      "        (sith): iSITH(ntau=400, tau_min=0.1, tau_max=4000, buff_max=6500, dt=1, k=35, g=0.0)\n",
      "        (conv): Conv2d(1, 35, kernel_size=(35, 23), stride=(1, 1), dilation=(1, 2), bias=False)\n",
      "        (maxp): MaxPool1d(kernel_size=356, stride=356, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (act_func): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (to_out): Linear(in_features=35, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "p = [400, 35, 23, 2]\n",
    "\n",
    "sp1 = dict(in_features=1, \n",
    "           tau_min=.1, tau_max=4000, buff_max=6500,\n",
    "           dt=1, ntau=p[0], k=p[1], g=0.0, ttype=ttype, \n",
    "           channels=35, kernel_width=p[2], dilation=p[3],\n",
    "           dropout=None, batch_norm=None)\n",
    "sp2 = dict(in_features=sp1['channels'], \n",
    "           tau_min=.1, tau_max=4000, buff_max=6500,\n",
    "           dt=1, ntau=p[0], k=p[1], g=0.0, ttype=ttype, \n",
    "           channels=35, kernel_width=p[2], dilation=p[3], \n",
    "           dropout=None, batch_norm=None)\n",
    "sp3 = dict(in_features=sp2['channels'], \n",
    "           tau_min=.1, tau_max=4000, buff_max=6500,\n",
    "           dt=1, ntau=p[0], k=p[1], g=0.0, ttype=ttype, \n",
    "           channels=35, kernel_width=p[2], dilation=p[3], \n",
    "           dropout=None, batch_norm=None)\n",
    "\n",
    "# TWO LAYERS\n",
    "layer_params = [sp1, sp2]#, sp3]\n",
    "\n",
    "\n",
    "model = SITHCon_Classifier(len(X), layer_params, act_func=nn.ReLU).cuda()\n",
    "model\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:20:39.067194Z",
     "start_time": "2021-05-03T01:20:28.413585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf9feef7634f148389282edcae4159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 5000\n",
    "Trainscale = 10\n",
    "device='cuda'\n",
    "batch_size = 8\n",
    "batches = int(np.ceil(43 / batch_size))\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "times_100 = 0\n",
    "for epoch_idx in progress_bar:\n",
    "    perfs = []\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx in range(batches):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        permute = np.arange(0, 43)\n",
    "        for i in range(0, int(min(len(X) - (batch_idx*batch_size), \n",
    "                              batch_size))\n",
    "                       ):\n",
    "            iv = X[permute[batch_idx*batch_size + i]]\n",
    "            iv = iv.unsqueeze(0).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            iv = iv.unsqueeze(-1)\n",
    "            iv = iv.repeat(1,1,1,1,Trainscale)\n",
    "            iv = iv.reshape(1,1,1,-1)\n",
    "            tv = Y[permute[batch_idx*batch_size + i]].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(iv)\n",
    "            loss += loss_func(out[:, -1, :],\n",
    "                             torch.cuda.LongTensor([tv]))\n",
    "            perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n",
    "                      tv).sum().item())\n",
    "        loss = loss / min(len(X) - (batch_idx*batch_size), \n",
    "                          batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #perfs = perfs[int(-loss_buffer_size/batch_size):]\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        #losses = losses[int(-loss_buffer_size/batch_size):]\n",
    "        \n",
    "        \n",
    "        s = \"{}:{:2} Loss: {:.4f}, Perf: {:.4f}\"\n",
    "        format_list = [epoch_idx, batch_idx, np.mean(losses), \n",
    "                       np.sum(perfs)/((len(perfs)))]\n",
    "        s = s.format(*format_list)\n",
    "        progress_bar.set_description(s)\n",
    "    if (np.sum(perfs)/((len(perfs))) == 1.0) & (np.mean(losses) < .11):\n",
    "        times_100 += 1\n",
    "        if times_100 >= 3:\n",
    "            break\n",
    "    else:\n",
    "        times_100 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T01:23:29.500713Z",
     "start_time": "2021-05-03T01:23:26.450075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.023255813953488372\n",
      "2 0.13953488372093023\n",
      "5 0.8372093023255814\n",
      "6 0.9767441860465116\n",
      "7 0.9767441860465116\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n",
      "30 0.9767441860465116\n",
      "40 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "evald = []\n",
    "evaldDict = {'perf':[],\n",
    "             'scale':[]}\n",
    "for nr in [1,2,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40]:\n",
    "#for nr in range(1,40,):\n",
    "    perfs = []\n",
    "    for batch_idx, iv in enumerate(X):\n",
    "        iv = iv.unsqueeze(0).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        iv = iv.unsqueeze(-1)\n",
    "        iv = iv.repeat(1,1,1,1,nr)\n",
    "        iv = iv.reshape(1,1,1,-1)\n",
    "        tv = Y[batch_idx].to(device)\n",
    "        out = model(iv)\n",
    "        loss = loss_func(out[:, -1, :],\n",
    "                         torch.cuda.LongTensor([tv]))\n",
    "\n",
    "\n",
    "        perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n",
    "                      tv).sum().item())\n",
    "    evaldDict['perf'].append(sum(perfs)/len(perfs))\n",
    "    evaldDict['scale'].append(nr)\n",
    "    print(nr, sum(perfs)/len(perfs))\n",
    "    evald.append([nr, sum(perfs)/(len(perfs)*1.0)])\n",
    "scale_perfs = pd.DataFrame(evaldDict)\n",
    "scale_perfs.to_pickle(join(\"perf\", \"sithcon_morse_test.dill\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
