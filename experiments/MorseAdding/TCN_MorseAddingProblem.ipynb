{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:11.423765Z",
     "start_time": "2021-05-12T14:25:11.253081Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:11.967687Z",
     "start_time": "2021-05-12T14:25:11.426139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set_context(\"poster\")\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "ttype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "ctype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "print(ttype)\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import gridspec\n",
    "from sithcon import SITHCon_Layer, _SITHCon_Core, iSITH\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import itertools\n",
    "from csv import DictWriter\n",
    "import os \n",
    "from os.path import join\n",
    "import glob\n",
    "ttype=torch.cuda.FloatTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from math import factorial\n",
    "import random\n",
    "from itertools import combinations_with_replacement as comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:11.971556Z",
     "start_time": "2021-05-12T14:25:11.968917Z"
    }
   },
   "outputs": [],
   "source": [
    "MORSE_CODE_DICT = {'1':'.----', '2':'..---', '3':'...--', \n",
    "                    '4':'....-', '5':'.....', '6':'-....', \n",
    "                    '7':'--...', '8':'---..', '9':'----.', \n",
    "                    '0':'-----', } \n",
    "\n",
    "# SHORTER\n",
    "MORSE_CODE_DICT = {'1':'.-', '2':'-...', \n",
    "                    '3':'-.-.', '4':'-..', '5':'.', \n",
    "                    '6':'..-.', '7':'--.', '8':'....', \n",
    "                    '9':'..', '0':'.---',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:11.985734Z",
     "start_time": "2021-05-12T14:25:11.972669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1 0 1 1 1 0 0 0] 1\n",
      "[1 1 1 0 1 0 1 0 1 0 0 0] 2\n",
      "[1 1 1 0 1 0 1 1 1 0 1 0 0 0] 3\n",
      "[1 1 1 0 1 0 1 0 0 0] 4\n",
      "[1 0 0 0] 5\n",
      "[1 0 1 0 1 1 1 0 1 0 0 0] 6\n",
      "[1 1 1 0 1 1 1 0 1 0 0 0] 7\n",
      "[1 0 1 0 1 0 1 0 0 0] 8\n",
      "[1 0 1 0 0 0] 9\n",
      "[1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0] 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print()\n",
    "morse_code_numpy = {key:np.array([int(x) for x in MORSE_CODE_DICT[key].replace('.', '10').replace('-', '1110')] + [0, 0])\n",
    "                    for key in MORSE_CODE_DICT.keys()}\n",
    "for k in morse_code_numpy.keys():\n",
    "    print(morse_code_numpy[k], k)\n",
    "subset = list(morse_code_numpy.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:12.113548Z",
     "start_time": "2021-05-12T14:25:11.986453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_item(samples=10, scale=5, target_scale=.1):\n",
    "    # We can tests 1s the same length as the items added together or 1 at the end only.\n",
    "    keys = morse_code_numpy.keys()\n",
    "    dim1 = []\n",
    "    dim2 = []\n",
    "    half = int(samples / 2)\n",
    "    added_indexes = [np.random.randint(half),  \n",
    "                     np.random.randint(half, samples)]\n",
    "    \n",
    "    answer = 0\n",
    "    for s in range(samples):\n",
    "        # Grab Random Morse Code Letter\n",
    "        k = random.sample(keys, 1)[0]\n",
    "        mcl = morse_code_numpy[k]\n",
    "        Mmcl = mcl.repeat(scale)\n",
    "        dim1.append(Mmcl[:, np.newaxis])\n",
    "        if s in added_indexes:\n",
    "            \n",
    "            # dim2.append(np.ones(Mmcl.shape[0])[:, np.newaxis])\n",
    "            temp = np.zeros(Mmcl.shape[0])[:, np.newaxis]\n",
    "            temp[-scale:] = 1.0\n",
    "            #temp[-1] = 1.0 # TRY THIS AT SOME POINT\n",
    "            dim2.append(temp)\n",
    "            answer += int(k)\n",
    "        else:\n",
    "            dim2.append(np.zeros(Mmcl.shape[0])[:, np.newaxis])\n",
    "    inp = np.concatenate([np.concatenate(dim1, axis=0),\n",
    "                          np.concatenate(dim2, axis=0)], axis=1)\n",
    "    \n",
    "    target = np.array([answer])\n",
    "    return inp, target*target_scale\n",
    "inp, tar = gen_item(5, 5, .1)\n",
    "print(inp.shape)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(inp.T,aspect='auto', interpolation='none')\n",
    "plt.title(\"Answer: {:.2f}\".format(tar[0]))\n",
    "plt.yticks([])\n",
    "plt.savefig(join('figs', 'adding_morse_example'), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:12.257458Z",
     "start_time": "2021-05-12T14:25:12.214501Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:12.982073Z",
     "start_time": "2021-05-12T14:25:12.972372Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_model(p):\n",
    "    model = TCN(2, 1, [25, 25, 25, 25,25,25,25,25], kernel_size=45, dropout=0.0).cuda()\n",
    "    return model\n",
    "\n",
    "def test_model(model, X, Y):\n",
    "    model.eval()\n",
    "    evald = []\n",
    "    evaldDict = {'test_perf':[],\n",
    "                 'rate':[],\n",
    "                 'tau_max':[],\n",
    "                 'ntau':[],\n",
    "                 'k':[]}\n",
    "    \n",
    "    # BIG NOTE\n",
    "    # BIG NOTE\n",
    "    # BIG NOTE\n",
    "    # Generate the test items once, use in all models at all scales. \n",
    "    model.eval()\n",
    "    evald = []\n",
    "    evaldDict = {'test_perf':[],\n",
    "                 'rate':[]}\n",
    "    for nr in range(1,20):\n",
    "        losses = []\n",
    "        for iv, tar in items:\n",
    "\n",
    "            iv = ttype(iv).unsqueeze(0).unsqueeze(0).transpose(-1,-2).unsqueeze(-1)\n",
    "            iv = iv.repeat(1,1,1,1,nr)\n",
    "            iv = iv.reshape(1,1,2,-1)\n",
    "            tv = torch.FloatTensor(tar).to(device)\n",
    "            out = model(iv)\n",
    "\n",
    "            loss = loss_func(out[:, -1, :],\n",
    "                                 tv)\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "        print(nr, np.mean(losses))\n",
    "        evaldDict['test_perf'].append(np.mean(losses))\n",
    "        evaldDict['rate'].append(nr)\n",
    "        evaldDict['tau_max'].append(model.sithcon_layers[0].sithcon.sith.tau_max)\n",
    "        evaldDict['ntau'].append(model.sithcon_layers[0].sithcon.sith.ntau)\n",
    "        evaldDict['k'].append(model.sithcon_layers[0].sithcon.sith.k)\n",
    "    return evaldDict\n",
    "\n",
    "def save_outcome(outcome, filename):\n",
    "    dat = pd.DataFrame(outcome)\n",
    "    dat.to_csv(join('perf',filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:14.935158Z",
     "start_time": "2021-05-12T14:25:14.929667Z"
    }
   },
   "outputs": [],
   "source": [
    "params = [\n",
    "           [None],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T14:25:16.436250Z",
     "start_time": "2021-05-12T14:25:15.509742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Weights: 425026\n",
      "TCN(\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(2, 25, kernel_size=(45,), stride=(1,), padding=(44,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(44,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(2, 25, kernel_size=(45,), stride=(1,), padding=(44,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(44,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(2, 25, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(88,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(88,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(88,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(88,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(176,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(176,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(176,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(176,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(352,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(352,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(352,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(352,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(704,), dilation=(16,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(704,), dilation=(16,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(704,), dilation=(16,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(704,), dilation=(16,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(1408,), dilation=(32,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(1408,), dilation=(32,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(1408,), dilation=(32,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(1408,), dilation=(32,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(2816,), dilation=(64,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(2816,), dilation=(64,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(2816,), dilation=(64,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(2816,), dilation=(64,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (7): TemporalBlock(\n",
      "        (conv1): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(5632,), dilation=(128,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(5632,), dilation=(128,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(5632,), dilation=(128,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "          (4): Conv1d(25, 25, kernel_size=(45,), stride=(1,), padding=(5632,), dilation=(128,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = gen_model(params[0])\n",
    "\n",
    "\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:50:55.896402Z",
     "start_time": "2021-05-12T14:25:31.166680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff0b84092684daa8e51a9f595f26bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bgj5hk/bin/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5ebfabdf136f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                               tv)\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "Trainscale = 5\n",
    "epochs = 500\n",
    "trials_per_epoch = 1000\n",
    "batch_size = 32\n",
    "device='cuda'\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "\n",
    "for epoch_idx in progress_bar:\n",
    "    perfs = []\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx in range(trials_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            iv, tar = gen_item(10, Trainscale, .1)\n",
    "            iv = ttype(iv).unsqueeze(0).transpose(-1,-2)\n",
    "            tv = ttype(tar)\n",
    "            out = model(iv)\n",
    "            loss += loss_func(out,\n",
    "                              tv)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        perfs.append(0)\n",
    "        #perfs = perfs[int(-loss_buffer_size/batch_size):]\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        #losses = losses[int(-loss_buffer_size/batch_size):]\n",
    "        \n",
    "        \n",
    "        s = \"{}:{:2} Loss: {:.4f}, Perf: {:.4f}\"\n",
    "        format_list = [epoch_idx, batch_idx, np.mean(losses), \n",
    "                       np.sum(perfs)/((len(perfs)))]\n",
    "        s = s.format(*format_list)\n",
    "        progress_bar.set_description(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:51:06.449121Z",
     "start_time": "2021-05-12T15:51:06.439344Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'TCN_MorseAdding_5122021.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:51:09.062210Z",
     "start_time": "2021-05-12T15:51:09.054008Z"
    }
   },
   "outputs": [],
   "source": [
    "items = np.load('generated_adding_morse.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T15:52:03.713792Z",
     "start_time": "2021-05-12T15:51:15.691742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9299978\n",
      "2 0.8890356\n",
      "3 0.73953456\n",
      "4 0.5510555\n",
      "5 6.921752e-05\n",
      "6 0.14487605\n",
      "7 0.24372695\n",
      "8 0.9595295\n",
      "9 3.2367554\n",
      "10 3.9829988\n",
      "11 3.2584848\n",
      "12 2.4428368\n",
      "13 2.399306\n",
      "14 3.1098676\n",
      "15 3.9704733\n",
      "16 4.7456784\n",
      "17 5.915663\n",
      "18 6.5673766\n",
      "19 6.8392315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BIG NOTE\n",
    "# BIG NOTE\n",
    "# BIG NOTE\n",
    "# Generate the test items once, use in all models at all scales. \n",
    "model.eval()\n",
    "evald = []\n",
    "evaldDict = {'test_perf':[],\n",
    "             'rate':[]}\n",
    "for nr in range(1,20):\n",
    "    losses = []\n",
    "    for iv, tar in items:\n",
    "\n",
    "        \n",
    "        iv = ttype(iv).unsqueeze(0).transpose(-1,-2).unsqueeze(-1)\n",
    "        iv = iv.repeat(1,1,1,nr)\n",
    "        iv = iv.reshape(1,2,-1)\n",
    "        tv = torch.FloatTensor(tar).to(device)\n",
    "        out = model(iv)\n",
    "        \n",
    "        loss = loss_func(out,\n",
    "                             tv)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "    print(nr, np.mean(losses))\n",
    "    evaldDict['test_perf'].append(np.mean(losses))\n",
    "    evaldDict['rate'].append(nr)\n",
    "    evald.append([nr, np.mean(losses)])\n",
    "scale_perfs = pd.DataFrame(evaldDict)\n",
    "scale_perfs.to_pickle(join(\"perf\", \"tcn_morseadding_test_5122021.dill\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7167px",
    "left": "1045.97px",
    "top": "52px",
    "width": "161.033px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
